{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Q-Learning mit OpenAI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Übung wird mit einem Jupyter-Notebook angegeben, kann aber auch in Rmarkdown übertragen werden.\n",
    "\n",
    "Ziel ist es, mit dem OpenAI Gym einen \"eigenen\" Agent zu erstellen.\n",
    "\n",
    "## Beispiel: Selbstfahrendes Taxi:\n",
    "\n",
    "Lasst uns eine Simulation von einem selbstfahrenden Taxi entwerfen. Das Hauptziel ist es, in einer vereinfachten Umgebung zu demonstrieren, wie man mit Hilfe von RL-Techniken einen effizienten und sicheren Ansatz zur Lösung dieses Problems entwickeln kann.\n",
    "\n",
    "Die Aufgabe der Smartcab ist es, den Fahrgast an einem Ort abzuholen und an einem anderen Ort abzusetzen. Hier sind ein paar Dinge, um die wir uns mit unserem Smartcab gerne kümmern würden:\n",
    "\n",
    "- Lasst den Beifahrer an der richtigen Stelle zurück.\n",
    "- Zeitersparnis für die Fahrgäste durch minimale Zeitersparnis beim Absetzen der Fahrgäste\n",
    "- Beachten Sie die Sicherheit der Fahrgäste und die Verkehrsregeln.\n",
    "\n",
    "Es gibt verschiedene Aspekte, die hier bei der Modellierung einer RL-Lösung für dieses Problem berücksichtigt werden müssen: Belohnungen, Zustände und Aktionen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Rewards\n",
    "Da der Agent (der imaginäre Fahrer) belohnungsmotiviert ist und lernen wird, wie man das Taxi durch Trial-Erfahrungen in der Umwelt steuert, müssen wir die Belohnungen und/oder Strafen und deren Höhe entsprechend festlegen. Hier ein paar Punkte, die zu beachten sind:\n",
    "\n",
    "- Der Agent sollte eine hohe positive Belohnung für einen erfolgreichen Dropoff erhalten, da dieses Verhalten sehr erwünscht ist.\n",
    "- Der Agent sollte bestraft werden, wenn er versucht, einen Passagier an falschen Orten abzusetzen.\n",
    "- Der Agent sollte eine leichte negative Belohnung dafür erhalten, dass er es nach jedem Zeitschritt nicht bis zum Ziel geschafft hat. \"Leicht\" negativ, weil wir es vorziehen würden, dass unser Agent zu spät kommt, anstatt falsche Schritte zu unternehmen und zu versuchen, das Ziel so schnell wie möglich zu erreichen.\n",
    "\n",
    "\n",
    "# 2. State Space\n",
    "Beim Reinforcement Learning begegnet der Agent einem Zustand und ergreift dann Maßnahmen entsprechend dem Zustand, in dem er sich befindet.\n",
    "\n",
    "Der State Space ist die Gesamtheit aller möglichen Situationen, in denen unser Taxi leben könnte. Der Zustand sollte nützliche Informationen enthalten, die der Agent benötigt, um die richtigen Maßnahmen zu ergreifen.\n",
    "\n",
    "Nehmen wir an, wir haben einen Trainingsbereich für unser Smartcab, in dem wir ihm beibringen, Menschen auf einem Parkplatz zu vier verschiedenen Orten (R, G, Y, B) zu transportieren:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![taxi_env](taxi_env.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nehmen wir an, Smartcab ist das einzige Fahrzeug auf diesem Parkplatz. Wir können den Parkplatz in ein 5x5 Raster aufteilen, was uns 25 mögliche Taxistandorte gibt. Diese 25 Standorte sind ein Teil unseres Staatsraums. Beachten Sie, dass der aktuelle Standortzustand unseres Taxis koordiniert ist (3, 1).\n",
    "\n",
    "Sie werden auch feststellen, dass es vier (4) Orte gibt, an denen wir einen Passagier abholen und absetzen können: R, G, Y, B oder ´[(0,0), (0,4), (4,0), (4,0), (4,3)]´ in Koordinaten (Reihe, Spalte). Unser illustrierter Passagier ist in Position Y und möchte zu Position **R** gehen.\n",
    "\n",
    "Wenn wir auch einen (1) zusätzlichen Fahrgastzustand innerhalb des Taxis berücksichtigen, können wir alle Kombinationen von Fahrgast- und Zielorten nehmen, um zu einer Gesamtzahl von Zuständen für unsere Taxiumgebung zu gelangen; es gibt vier (4) Ziele und fünf (4 + 1) Fahrgastziele.\n",
    "\n",
    "So hat unsere Taxiumgebung $5×5×5×5×4=500$ mögliche Zustände.\n",
    "\n",
    "Der Agent trifft auf einen der 500 Zustände und er ergreift eine Aktion. Die Aktion in unserem Fall kann sein, sich in eine Richtung zu bewegen oder sich zu entscheiden, einen Fahrgast abzuholen oder abzusetzen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Action Space \n",
    "Mit anderen Worten, wir haben sechs mögliche Aktionen:\n",
    "\n",
    "1. `south`\n",
    "2. `north` \n",
    "3. `east` \n",
    "4. `west` \n",
    "5. `pickup` \n",
    "6. `dropoff` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dies ist der Action Space: der Satz aller Aktionen, die unser Agent in einem bestimmten Zustand ausführen kann.\n",
    "\n",
    "Sie werden in der obigen Abbildung feststellen, dass das Taxi in bestimmten Zuständen aufgrund von Mauern bestimmte Aktionen nicht ausführen kann. Im Code der Umgebung geben wir einfach eine -1 Strafe für jeden Mauerstoß und das Taxi bewegt sich nirgendwo hin. Dies wird nur Strafen mit sich bringen, die das Taxi dazu bringen, eine Umgehung der Mauer in Betracht zu ziehen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup der Umgebung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"Taxi-v2\").env\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Kern-Schnittstelle des Gym ist env, das ist die Unified Environment Interface. Nachfolgend sind die env-Methoden aufgeführt, die uns sehr hilfreich sein könnten:\n",
    "\n",
    "- `env.reset`: Setzt die Umgebung zurück und gibt einen zufälligen Ausgangszustand zurück.\n",
    "- `env.step(action)`: Schieben Sie die Umgebung um einen Zeitschritt nach vorne. Retouren\n",
    "  - observation: Umweltbeobachtungen\n",
    "  - reward: Ob Ihre Aktion nützlich war oder nicht.\n",
    "  - done: Zeigt an, ob wir einen Passagier, auch eine Episode genannt, erfolgreich abgeholt und abgesetzt haben.\n",
    "  - info: Zusätzliche Informationen wie Performance und Latenzzeiten für Debuggingzwecke\n",
    "- `env.render`: Stellt einen Rahmen der Umgebung dar (hilfreich bei der Visualisierung der Umgebung)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R:\u001b[43m \u001b[0m| : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "Action Space Discrete(6)\n",
      "State Space Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "env.reset() # reset environment to a new, random state\n",
    "env.render()\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ein State:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 328\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 428, -1, False)],\n",
       " 1: [(1.0, 228, -1, False)],\n",
       " 2: [(1.0, 348, -1, False)],\n",
       " 3: [(1.0, 328, -1, False)],\n",
       " 4: [(1.0, 328, -10, False)],\n",
       " 5: [(1.0, 328, -10, False)]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[328] # Reward Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lösung ohne Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 3939\n",
      "Penalties incurred: 1329\n"
     ]
    }
   ],
   "source": [
    "env.s = 328  # set environment to illustration's state\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "sum_penalties = 0\n",
    "sum_rewards = 0\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward,\n",
    "        }\n",
    "    )\n",
    "    epochs += 1\n",
    "    sum_penalties += penalties\n",
    "    sum_rewards += reward\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 3939\n",
      "State: 0\n",
      "Action: 5\n",
      "Reward: 20\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.001)\n",
    "       \n",
    "print_frames(frames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 1 : Implementierung von Q-Learning in python (5 Punkte)\n",
    "\n",
    "**Tipps:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "\n",
    "#Hypoparameter:\n",
    "\n",
    "#alpha\n",
    "a = 0.1\n",
    "#gamma\n",
    "g = 0.7\n",
    "#epsilon\n",
    "e = 0.1\n",
    "\n",
    "for i in range(0,200000):\n",
    "    state = env.reset()\n",
    "    epochsRL = 0\n",
    "    penalties = 0\n",
    "    reward = 0\n",
    "    \n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        if random.uniform(0,1) < e:\n",
    "            # Exploration of the enviroment\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # exploit (verwerten) from already learned values\n",
    "            # argmax returns indices of the max element of the array in a particular axis\n",
    "            action = np.argmax(q_table[state])\n",
    "            \n",
    "        state_next, reward, done, info = env.step(action)\n",
    "        \n",
    "        value_before = q_table[state, action]\n",
    "        next_max = np.max(q_table[state_next])\n",
    "        \n",
    "        #Q-Learning Formel: \n",
    "        value_next = (1-a)* value_before + a * (reward + g * next_max)\n",
    "        q_table[state, action] = value_next\n",
    "        \n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "        \n",
    "        state = state_next\n",
    "        epochsRL += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluierung \n",
    " \n",
    "Bei der Evaluierung wird immer der beste Q-value gewählt - eine Exploration findet nicht mehr statt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_epochs_RL = 0\n",
    "sum_penalties_RL = 0\n",
    "sum_rewards_RL = 0\n",
    "\n",
    "episodesRL = 100\n",
    "\n",
    "for i in range(episodesRL):\n",
    "    state = env.reset()\n",
    "    epochsRL, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochsRL += 1\n",
    "        sum_rewards_RL +=reward\n",
    "    sum_penalties_RL += penalties\n",
    "    sum_epochs_RL += epochsRL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 2: Vergleich von Q-Learning (10 Punkte)\n",
    "\n",
    "\n",
    "Wir bewerten unsere Agenten anhand der folgenden Kennzahlen,\n",
    "\n",
    "- **Durchschnittliche Anzahl der Strafen pro Episode**: Je kleiner die Zahl, desto besser die Leistung unseres Agenten. Im Idealfall möchten wir, dass diese Kennzahl Null oder sehr nahe bei Null liegt.\n",
    "- **Durchschnittliche Anzahl der Zeitschritte pro Fahrt**: Wir wollen auch eine kleine Anzahl von Zeitschritten pro Episode, da wir möchten, dass unser Agent minimale Schritte (d.h. den kürzesten Weg) unternimmt, um das Ziel zu erreichen.\n",
    "- **Durchschnittliche Belohnungen pro Zug**: Je größer die Belohnung, desto besser ist es, dass der Agent das Richtige tut. Deshalb ist die Entscheidung über die Belohnung ein wichtiger Teil des Verstärkungslernens. In unserem Fall, da sowohl Zeitschritte als auch Strafen negativ belohnt werden, würde eine höhere durchschnittliche Belohnung bedeuten, dass der Agent das Ziel so schnell wie möglich mit den geringsten Strafen erreicht\".\n",
    "\n",
    "\n",
    "Erstellen Sie eine Tabelle und vergleichen Sie Q-Learning mit dem \"simplen\" Ansatz von oben.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vergleich der zwei Modelle (no-RL vs. RL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model OHNE RL</th>\n",
       "      <th>Model MIT Q-Learning (RL)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Durchschnittliche Anzahl der Strafen pro Episode:</th>\n",
       "      <td>658.003808</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Durchschnittliche Anzahl der Zeitschritte pro Fahrt:</th>\n",
       "      <td>3939.000000</td>\n",
       "      <td>12.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Durchschnittliche Belohnungen pro Zug:</th>\n",
       "      <td>-4.031226</td>\n",
       "      <td>0.639344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Model OHNE RL  \\\n",
       "Durchschnittliche Anzahl der Strafen pro Episode:      658.003808   \n",
       "Durchschnittliche Anzahl der Zeitschritte pro F...    3939.000000   \n",
       "Durchschnittliche Belohnungen pro Zug:                  -4.031226   \n",
       "\n",
       "                                                    Model MIT Q-Learning (RL)  \n",
       "Durchschnittliche Anzahl der Strafen pro Episode:                    0.000000  \n",
       "Durchschnittliche Anzahl der Zeitschritte pro F...                  12.810000  \n",
       "Durchschnittliche Belohnungen pro Zug:                               0.639344  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "d = {'Model OHNE RL':[\n",
    "        (sum_penalties/epochs),\n",
    "        (epochs),\n",
    "        (sum_rewards/epochs)],\n",
    "     'Model MIT Q-Learning (RL)':[  \n",
    "        (sum_penalties_RL/episodesRL),\n",
    "        (sum_epochs_RL/episodesRL),\n",
    "        (sum_rewards_RL/sum_epochs_RL)]}\n",
    "\n",
    "dataframe = pd.DataFrame(data=d)\n",
    "dataframe = dataframe.rename(index={0:'Durchschnittliche Anzahl der Strafen pro Episode:',\n",
    "                                    1:'Durchschnittliche Anzahl der Zeitschritte pro Fahrt:',\n",
    "                                    2:'Durchschnittliche Belohnungen pro Zug:'})\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erläuterung\n",
    "Bei der Gegenüberstellung der zwei Modelle fällt auf, dass Reinforcment Learning (RL) insgesamt besser abschneidet, als das Model ohne RL. Die durchschnittliche Anzahl der Strafen pro Episode konnte in Richtung 0 gebracht werden. die durchschnittliche Anzahl der Zeitschritte hat sich mit dem Q-Learning deutlich reduziert. Die durchschnittliche Belohnung pro Zug konnte ebenfalls verbessert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 3: Hyperparameter (10 Punkte)\n",
    "\n",
    "Die Werte von `alpha`, `gamma` und `epsilon` basierten hauptsächlich auf Intuition und etwas \"Hit and Trial\", aber es gibt bessere Möglichkeiten, gute Werte zu finden.\n",
    "\n",
    "Im Idealfall sollten alle drei im Laufe der Zeit abnehmen, denn wenn der Agent weiter lernt, baut er tatsächlich widerstandsfähigere Vorgänger auf;\n",
    "\n",
    "- *α*: (die Lernrate) sollte abnehmen, da Sie immer mehr an einer immer größeren Wissensbasis gewinnen.\n",
    "- *γ*: Wenn Sie der Deadline immer näher kommen, sollte Ihre Präferenz für eine kurzfristige Belohnung steigen, da Sie nicht lange genug dabei sein werden, um die langfristige Belohnung zu erhalten, was bedeutet, dass Ihr Gamma sinken sollte.\n",
    "- *ϵ*: Während wir unsere Strategie entwickeln, haben wir weniger Bedarf an Exploration und mehr Ausbeutung, um mehr Nutzen aus unserer Politik zu ziehen, so dass mit zunehmender Anzahl der Versuche epsilon abnehmen sollte.\n",
    "\n",
    "Wie können die Hyperparameter \"gesucht\" werden?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Hyperparameter können mithilfe einer Suchfunktion (wie z.B: bei einer Rastersuche) erstellt werden. Mithilfe der Suchfunktion werden jene Werte für `alpha`, `gamma` und `epsilon` gewählt, die zu einem optimalen Verhältnis von Belohnung/Zeitschritten führen. So können die Parameter so gewählt werden, dass die maximale Belohnung so schnell wie möglich erreicht wird. Auch die Anzahl der Strafen spielt eine wichtige Rolle: Strafen sollen nicht in Kauf genommen werden, nur damit das Ziel schneller erreicht wird. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
